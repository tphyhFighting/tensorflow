卷积神经网络中的卷积核是学习得到的:一开始卷积核(参数w和b)都被初始化成很小的随机值。LeCun和Bengio教授的文章中建议在处理图像问题时，可以选择将w和b按照~U(-sqrt(3/k),sqrt(3/k))初始化.其中k是w和b的连接总数.假如滤波器的大小为5*5，那么k为25，U表示均匀分布，sqrt()为平方根运算。这个都是从经验出发的建议，并没有很明确的理论依据。
在使用训练数据对网络进行BP训练时，w和b的值都会往局部最优的方向更新，直至算法收敛.
所以卷积神经网络中的卷积核是从训练数据中学习得到的，为使得算法正常运行，需要给定一个初始值.
权重初始化1：不能全部是0,经过适当的数据预处理之后我们可以合理的认为大约有一半的权重是正，另一半是负，我们可以认为它的均值可能为0，但绝对不能将其全部都设为0，严格来说不能把所有权重都设置为相同的值。
2：反向传播的梯度大小与和权重大小成正比，如果是没有规律的小型网络还可以这样初始化，但是经过几层网络之后会产生激活值的费均匀分布，
3：经验告诉我们如果初始时每个单元的输出都有着相似的分布会使得收敛速度增大。
